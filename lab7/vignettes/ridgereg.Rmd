---
title: "Ridge Regression"
author: "Clara Schartner and Araya Eamrurksiri"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
```{r, echo=FALSE}
data("iris")
data("faithful")
```
## Ridge Regression
The Package `lab7` includes a function called `ridgreg` with which it is possible to do a ridge regression on data.
$\hat \beta^{ridge}=(X´X+ \lambda I)^{-1} X´y$ 
## Example
The use of the function is as can be seen below:
```{r}
library(lab7)
result<-ridgereg(eruptions~waiting,faithful)
result
```
## Further features
The function `predict` can be used to extract all predicted values.
```{r}
head(predict(result))
```
The function `coef` extracts all coefficients.
```{r}
coef(result)
```
And the function `print` prints both the formula used and the coefficients.
```{r}
print(result)
```

##Predictive model using caret library
Load the `BostonHousing` data found in the `library(mlbenchthe)` and divide the dataset into two part, training and testing dataset. Traning dataset has 75% of the original data while testing dataset has 25%.
```{r}
library(caret)
library(mlbench)
data("BostonHousing")
BostonHousing$chas <- as.numeric(BostonHousing$chas)-1 ##CHECK!!
intrain<-createDataPartition(BostonHousing$crim, p=0.75, list=FALSE)
training<-BostonHousing[intrain,]
testing<-BostonHousing[-intrain,]
```

The first step is to fit a linear regression model on the training dataset.
```{r}
set.seed(3245)
lm<-train(crim ~ .,data = training, method = "lm")
lm
```

Then, using a different method which is a linear regression model with forward selection to fit the training dataset.
```{r}
set.seed(3245)
lm.forward<-train(crim ~ .,data = training, method = "leapForward")
lm.forward
```

From this two model, `lm` and `lm.forward`,we can see that using a linear regression model, or `lm`, to fit the dataset is slightly better as it gives a smaller RMSE.

Moreover, it shows that a linear regression model with forward selection, or `lm.forward`, has the optimal value when using 2 parameters.

```{r}
# pre.forward<-predict(newdata=training, lm.forward)
# plot(training$crim,col="red")
# lines(pre.forward)
```

Next step is to include the custom method which is "ridge regression" into the caret package.
```{r}
ridge <- list(type=c("Regression"), 
                 library="lab7",
                 loop=NULL)

ridge$parameters <- data.frame(parameter="lambda",
                  class="numeric",
                  label="lambda")

ridge$grid <- function(y,x, len=NULL, search="grid"){
 data.frame(lambda=c(0.01,0.5,1,3,8,30))
}

ridge$fit <- function(y, x, lambda, param, lev, last, classProbs, ...){
   lab7::ridgereg1(y=y, x=x, lambda=param$lambda) 
}

ridge$prob<-list(NULL)

ridge$predict<-function(modelFit, newdata,preProc=NULL, submodels=NULL){
  predict(modelFit, newdata)
}

ridge$sort<-function (x) x[order(-x$lambda), ]

ridge$label<-"Ridge"
``` 

Now, we can fit a ridge regression for this training dataset.
```{r}
fitControl <- trainControl(
  method = "repeatedcv",
  number = 10,
  repeats = 10)

set.seed(3245)
lm.ridge <- train(y=training$crim, x=training[,-1], method=ridge, trControl = fitControl)
lm.ridge            
```
We can tell from looking at this result that the lambda which gives the smallest RMSE is 30.


Finally, evaluate the models on the test dataset. Following are the results:
```{r}
set.seed(3245)
lm.test <- train(crim ~ .,data = testing, method = "lm")
lm.test

lm.forward.test <- train(crim ~ .,data = testing, method = "leapForward")
lm.forward.test

lm.ridge.test <- train(y=testing$crim, x=testing[,-1], method=ridge, trControl = fitControl)
lm.ridge.test
```

This result shows that when fitting the three models, `lm.test`, `lm.forward.test`, and `lm.ridge.test`, with the test dataset, the ridge regression model gives us the best result. The ridge regression with the best hyperparameter value, lambda is 30, has the smallest RMSE.