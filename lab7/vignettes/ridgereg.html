<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="author" content="Clara Schartner and Araya Eamrurksiri" />

<meta name="date" content="2015-10-19" />

<title>Ridge Regression</title>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; }
code > span.dt { color: #902000; }
code > span.dv { color: #40a070; }
code > span.bn { color: #40a070; }
code > span.fl { color: #40a070; }
code > span.ch { color: #4070a0; }
code > span.st { color: #4070a0; }
code > span.co { color: #60a0b0; font-style: italic; }
code > span.ot { color: #007020; }
code > span.al { color: #ff0000; font-weight: bold; }
code > span.fu { color: #06287e; }
code > span.er { color: #ff0000; font-weight: bold; }
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<link href="data:text/css,body%20%7B%0A%20%20background%2Dcolor%3A%20%23fff%3B%0A%20%20margin%3A%201em%20auto%3B%0A%20%20max%2Dwidth%3A%20700px%3B%0A%20%20overflow%3A%20visible%3B%0A%20%20padding%2Dleft%3A%202em%3B%0A%20%20padding%2Dright%3A%202em%3B%0A%20%20font%2Dfamily%3A%20%22Open%20Sans%22%2C%20%22Helvetica%20Neue%22%2C%20Helvetica%2C%20Arial%2C%20sans%2Dserif%3B%0A%20%20font%2Dsize%3A%2014px%3B%0A%20%20line%2Dheight%3A%201%2E35%3B%0A%7D%0A%0A%23header%20%7B%0A%20%20text%2Dalign%3A%20center%3B%0A%7D%0A%0A%23TOC%20%7B%0A%20%20clear%3A%20both%3B%0A%20%20margin%3A%200%200%2010px%2010px%3B%0A%20%20padding%3A%204px%3B%0A%20%20width%3A%20400px%3B%0A%20%20border%3A%201px%20solid%20%23CCCCCC%3B%0A%20%20border%2Dradius%3A%205px%3B%0A%0A%20%20background%2Dcolor%3A%20%23f6f6f6%3B%0A%20%20font%2Dsize%3A%2013px%3B%0A%20%20line%2Dheight%3A%201%2E3%3B%0A%7D%0A%20%20%23TOC%20%2Etoctitle%20%7B%0A%20%20%20%20font%2Dweight%3A%20bold%3B%0A%20%20%20%20font%2Dsize%3A%2015px%3B%0A%20%20%20%20margin%2Dleft%3A%205px%3B%0A%20%20%7D%0A%0A%20%20%23TOC%20ul%20%7B%0A%20%20%20%20padding%2Dleft%3A%2040px%3B%0A%20%20%20%20margin%2Dleft%3A%20%2D1%2E5em%3B%0A%20%20%20%20margin%2Dtop%3A%205px%3B%0A%20%20%20%20margin%2Dbottom%3A%205px%3B%0A%20%20%7D%0A%20%20%23TOC%20ul%20ul%20%7B%0A%20%20%20%20margin%2Dleft%3A%20%2D2em%3B%0A%20%20%7D%0A%20%20%23TOC%20li%20%7B%0A%20%20%20%20line%2Dheight%3A%2016px%3B%0A%20%20%7D%0A%0Atable%20%7B%0A%20%20margin%3A%201em%20auto%3B%0A%20%20border%2Dwidth%3A%201px%3B%0A%20%20border%2Dcolor%3A%20%23DDDDDD%3B%0A%20%20border%2Dstyle%3A%20outset%3B%0A%20%20border%2Dcollapse%3A%20collapse%3B%0A%7D%0Atable%20th%20%7B%0A%20%20border%2Dwidth%3A%202px%3B%0A%20%20padding%3A%205px%3B%0A%20%20border%2Dstyle%3A%20inset%3B%0A%7D%0Atable%20td%20%7B%0A%20%20border%2Dwidth%3A%201px%3B%0A%20%20border%2Dstyle%3A%20inset%3B%0A%20%20line%2Dheight%3A%2018px%3B%0A%20%20padding%3A%205px%205px%3B%0A%7D%0Atable%2C%20table%20th%2C%20table%20td%20%7B%0A%20%20border%2Dleft%2Dstyle%3A%20none%3B%0A%20%20border%2Dright%2Dstyle%3A%20none%3B%0A%7D%0Atable%20thead%2C%20table%20tr%2Eeven%20%7B%0A%20%20background%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0A%0Ap%20%7B%0A%20%20margin%3A%200%2E5em%200%3B%0A%7D%0A%0Ablockquote%20%7B%0A%20%20background%2Dcolor%3A%20%23f6f6f6%3B%0A%20%20padding%3A%200%2E25em%200%2E75em%3B%0A%7D%0A%0Ahr%20%7B%0A%20%20border%2Dstyle%3A%20solid%3B%0A%20%20border%3A%20none%3B%0A%20%20border%2Dtop%3A%201px%20solid%20%23777%3B%0A%20%20margin%3A%2028px%200%3B%0A%7D%0A%0Adl%20%7B%0A%20%20margin%2Dleft%3A%200%3B%0A%7D%0A%20%20dl%20dd%20%7B%0A%20%20%20%20margin%2Dbottom%3A%2013px%3B%0A%20%20%20%20margin%2Dleft%3A%2013px%3B%0A%20%20%7D%0A%20%20dl%20dt%20%7B%0A%20%20%20%20font%2Dweight%3A%20bold%3B%0A%20%20%7D%0A%0Aul%20%7B%0A%20%20margin%2Dtop%3A%200%3B%0A%7D%0A%20%20ul%20li%20%7B%0A%20%20%20%20list%2Dstyle%3A%20circle%20outside%3B%0A%20%20%7D%0A%20%20ul%20ul%20%7B%0A%20%20%20%20margin%2Dbottom%3A%200%3B%0A%20%20%7D%0A%0Apre%2C%20code%20%7B%0A%20%20background%2Dcolor%3A%20%23f7f7f7%3B%0A%20%20border%2Dradius%3A%203px%3B%0A%20%20color%3A%20%23333%3B%0A%7D%0Apre%20%7B%0A%20%20white%2Dspace%3A%20pre%2Dwrap%3B%20%20%20%20%2F%2A%20Wrap%20long%20lines%20%2A%2F%0A%20%20border%2Dradius%3A%203px%3B%0A%20%20margin%3A%205px%200px%2010px%200px%3B%0A%20%20padding%3A%2010px%3B%0A%7D%0Apre%3Anot%28%5Bclass%5D%29%20%7B%0A%20%20background%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0A%0Acode%20%7B%0A%20%20font%2Dfamily%3A%20Consolas%2C%20Monaco%2C%20%27Courier%20New%27%2C%20monospace%3B%0A%20%20font%2Dsize%3A%2085%25%3B%0A%7D%0Ap%20%3E%20code%2C%20li%20%3E%20code%20%7B%0A%20%20padding%3A%202px%200px%3B%0A%7D%0A%0Adiv%2Efigure%20%7B%0A%20%20text%2Dalign%3A%20center%3B%0A%7D%0Aimg%20%7B%0A%20%20background%2Dcolor%3A%20%23FFFFFF%3B%0A%20%20padding%3A%202px%3B%0A%20%20border%3A%201px%20solid%20%23DDDDDD%3B%0A%20%20border%2Dradius%3A%203px%3B%0A%20%20border%3A%201px%20solid%20%23CCCCCC%3B%0A%20%20margin%3A%200%205px%3B%0A%7D%0A%0Ah1%20%7B%0A%20%20margin%2Dtop%3A%200%3B%0A%20%20font%2Dsize%3A%2035px%3B%0A%20%20line%2Dheight%3A%2040px%3B%0A%7D%0A%0Ah2%20%7B%0A%20%20border%2Dbottom%3A%204px%20solid%20%23f7f7f7%3B%0A%20%20padding%2Dtop%3A%2010px%3B%0A%20%20padding%2Dbottom%3A%202px%3B%0A%20%20font%2Dsize%3A%20145%25%3B%0A%7D%0A%0Ah3%20%7B%0A%20%20border%2Dbottom%3A%202px%20solid%20%23f7f7f7%3B%0A%20%20padding%2Dtop%3A%2010px%3B%0A%20%20font%2Dsize%3A%20120%25%3B%0A%7D%0A%0Ah4%20%7B%0A%20%20border%2Dbottom%3A%201px%20solid%20%23f7f7f7%3B%0A%20%20margin%2Dleft%3A%208px%3B%0A%20%20font%2Dsize%3A%20105%25%3B%0A%7D%0A%0Ah5%2C%20h6%20%7B%0A%20%20border%2Dbottom%3A%201px%20solid%20%23ccc%3B%0A%20%20font%2Dsize%3A%20105%25%3B%0A%7D%0A%0Aa%20%7B%0A%20%20color%3A%20%230033dd%3B%0A%20%20text%2Ddecoration%3A%20none%3B%0A%7D%0A%20%20a%3Ahover%20%7B%0A%20%20%20%20color%3A%20%236666ff%3B%20%7D%0A%20%20a%3Avisited%20%7B%0A%20%20%20%20color%3A%20%23800080%3B%20%7D%0A%20%20a%3Avisited%3Ahover%20%7B%0A%20%20%20%20color%3A%20%23BB00BB%3B%20%7D%0A%20%20a%5Bhref%5E%3D%22http%3A%22%5D%20%7B%0A%20%20%20%20text%2Ddecoration%3A%20underline%3B%20%7D%0A%20%20a%5Bhref%5E%3D%22https%3A%22%5D%20%7B%0A%20%20%20%20text%2Ddecoration%3A%20underline%3B%20%7D%0A%0A%2F%2A%20Class%20described%20in%20https%3A%2F%2Fbenjeffrey%2Ecom%2Fposts%2Fpandoc%2Dsyntax%2Dhighlighting%2Dcss%0A%20%20%20Colours%20from%20https%3A%2F%2Fgist%2Egithub%2Ecom%2Frobsimmons%2F1172277%20%2A%2F%0A%0Acode%20%3E%20span%2Ekw%20%7B%20color%3A%20%23555%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%2F%2A%20Keyword%20%2A%2F%0Acode%20%3E%20span%2Edt%20%7B%20color%3A%20%23902000%3B%20%7D%20%2F%2A%20DataType%20%2A%2F%0Acode%20%3E%20span%2Edv%20%7B%20color%3A%20%2340a070%3B%20%7D%20%2F%2A%20DecVal%20%28decimal%20values%29%20%2A%2F%0Acode%20%3E%20span%2Ebn%20%7B%20color%3A%20%23d14%3B%20%7D%20%2F%2A%20BaseN%20%2A%2F%0Acode%20%3E%20span%2Efl%20%7B%20color%3A%20%23d14%3B%20%7D%20%2F%2A%20Float%20%2A%2F%0Acode%20%3E%20span%2Ech%20%7B%20color%3A%20%23d14%3B%20%7D%20%2F%2A%20Char%20%2A%2F%0Acode%20%3E%20span%2Est%20%7B%20color%3A%20%23d14%3B%20%7D%20%2F%2A%20String%20%2A%2F%0Acode%20%3E%20span%2Eco%20%7B%20color%3A%20%23888888%3B%20font%2Dstyle%3A%20italic%3B%20%7D%20%2F%2A%20Comment%20%2A%2F%0Acode%20%3E%20span%2Eot%20%7B%20color%3A%20%23007020%3B%20%7D%20%2F%2A%20OtherToken%20%2A%2F%0Acode%20%3E%20span%2Eal%20%7B%20color%3A%20%23ff0000%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%2F%2A%20AlertToken%20%2A%2F%0Acode%20%3E%20span%2Efu%20%7B%20color%3A%20%23900%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%2F%2A%20Function%20calls%20%2A%2F%20%0Acode%20%3E%20span%2Eer%20%7B%20color%3A%20%23a61717%3B%20background%2Dcolor%3A%20%23e3d2d2%3B%20%7D%20%2F%2A%20ErrorTok%20%2A%2F%0A%0A" rel="stylesheet" type="text/css" />

</head>

<body>



<div id="header">
<h1 class="title">Ridge Regression</h1>
<h4 class="author"><em>Clara Schartner and Araya Eamrurksiri</em></h4>
<h4 class="date"><em>2015-10-19</em></h4>
</div>


<div id="ridge-regression" class="section level2">
<h2>Ridge Regression</h2>
<p>The Package <code>lab7</code> includes a function called <code>ridgreg</code> with which it is possible to do a ridge regression on data. <span class="math">\(\hat \beta^{ridge}=(X´X+ \lambda I)^{-1} X´y\)</span> ## Example The use of the function is as can be seen below:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(lab7)
result&lt;-<span class="kw">ridgereg</span>(eruptions~waiting,faithful)
result</code></pre>
<pre><code>## Call:
## ridgereg(formula = eruptions ~ waiting, data = faithful)
## 
## Coefficients:
## (Intercept)     waiting 
## -1.85471086  0.07536527</code></pre>
</div>
<div id="further-features" class="section level2">
<h2>Further features</h2>
<p>The function <code>predict</code> can be used to extract all predicted values.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(<span class="kw">predict</span>(result))</code></pre>
<pre><code>## [1] 4.099145 2.215014 3.722319 2.817936 4.551337 2.290379</code></pre>
<p>The function <code>coef</code> extracts all coefficients.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">coef</span>(result)</code></pre>
<pre><code>## (Intercept)     waiting 
## -1.85471086  0.07536527</code></pre>
<p>And the function <code>print</code> prints both the formula used and the coefficients.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(result)</code></pre>
<pre><code>## Call:
## ridgereg(formula = eruptions ~ waiting, data = faithful)
## 
## Coefficients:
## (Intercept)     waiting 
## -1.85471086  0.07536527</code></pre>
</div>
<div id="predictive-model-using-caret-library" class="section level2">
<h2>Predictive model using caret library</h2>
<p>Load the <code>BostonHousing</code> data found in the <code>library(mlbenchthe)</code> and divide the dataset into two part, training and testing dataset. Traning dataset has 75% of the original data while testing dataset has 25%.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(caret)
<span class="kw">library</span>(mlbench)
<span class="kw">data</span>(<span class="st">&quot;BostonHousing&quot;</span>)
BostonHousing$chas &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(BostonHousing$chas)-<span class="dv">1</span> ##CHECK!!
intrain&lt;-<span class="kw">createDataPartition</span>(BostonHousing$crim, <span class="dt">p=</span><span class="fl">0.75</span>, <span class="dt">list=</span><span class="ot">FALSE</span>)
training&lt;-BostonHousing[intrain,]
testing&lt;-BostonHousing[-intrain,]</code></pre>
<p>The first step is to fit a linear regression model on the training dataset.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">3245</span>)
lm&lt;-<span class="kw">train</span>(crim ~<span class="st"> </span>.,<span class="dt">data =</span> training, <span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>)
lm</code></pre>
<pre><code>## Linear Regression 
## 
## 382 samples
##  13 predictor
## 
## No pre-processing
## Resampling: Bootstrapped (25 reps) 
## Summary of sample sizes: 382, 382, 382, 382, 382, 382, ... 
## Resampling results
## 
##   RMSE      Rsquared   RMSE SD   Rsquared SD
##   7.357237  0.4313459  1.957751  0.1204801  
## 
## </code></pre>
<p>Then, using a different method which is a linear regression model with forward selection to fit the training dataset.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">3245</span>)
lm.forward&lt;-<span class="kw">train</span>(crim ~<span class="st"> </span>.,<span class="dt">data =</span> training, <span class="dt">method =</span> <span class="st">&quot;leapForward&quot;</span>)</code></pre>
<pre><code>## Lade nötiges Paket: leaps</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">lm.forward</code></pre>
<pre><code>## Linear Regression with Forward Selection 
## 
## 382 samples
##  13 predictor
## 
## No pre-processing
## Resampling: Bootstrapped (25 reps) 
## Summary of sample sizes: 382, 382, 382, 382, 382, 382, ... 
## Resampling results across tuning parameters:
## 
##   nvmax  RMSE      Rsquared   RMSE SD   Rsquared SD
##   2      7.369008  0.4225566  2.046902  0.1274502  
##   3      7.379373  0.4219193  2.017779  0.1229588  
##   4      7.380401  0.4226003  2.016916  0.1233698  
## 
## RMSE was used to select the optimal model using  the smallest value.
## The final value used for the model was nvmax = 2.</code></pre>
<p>From this two model, <code>lm</code> and <code>lm.forward</code>,we can see that using a linear regression model, or <code>lm</code>, to fit the dataset is slightly better as it gives a smaller RMSE.</p>
<p>Moreover, it shows that a linear regression model with forward selection, or <code>lm.forward</code>, has the optimal value when using 2 parameters.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># pre.forward&lt;-predict(newdata=training, lm.forward)</span>
<span class="co"># plot(training$crim,col=&quot;red&quot;)</span>
<span class="co"># lines(pre.forward)</span></code></pre>
<p>Next step is to include the custom method which is “ridge regression” into the caret package.</p>
<pre class="sourceCode r"><code class="sourceCode r">ridge &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">type=</span><span class="kw">c</span>(<span class="st">&quot;Regression&quot;</span>), 
                 <span class="dt">library=</span><span class="st">&quot;lab7&quot;</span>,
                 <span class="dt">loop=</span><span class="ot">NULL</span>)

ridge$parameters &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">parameter=</span><span class="st">&quot;lambda&quot;</span>,
                  <span class="dt">class=</span><span class="st">&quot;numeric&quot;</span>,
                  <span class="dt">label=</span><span class="st">&quot;lambda&quot;</span>)

ridge$grid &lt;-<span class="st"> </span>function(y,x, <span class="dt">len=</span><span class="ot">NULL</span>, <span class="dt">search=</span><span class="st">&quot;grid&quot;</span>){
 <span class="kw">data.frame</span>(<span class="dt">lambda=</span><span class="kw">c</span>(<span class="fl">0.01</span>,<span class="fl">0.5</span>,<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">8</span>,<span class="dv">30</span>))
}

ridge$fit &lt;-<span class="st"> </span>function(y, x, lambda, param, lev, last, classProbs, ...){
   lab7::<span class="kw">ridgereg1</span>(<span class="dt">y=</span>y, <span class="dt">x=</span>x, <span class="dt">lambda=</span>param$lambda) 
}

ridge$prob&lt;-<span class="kw">list</span>(<span class="ot">NULL</span>)

ridge$predict&lt;-function(modelFit, newdata,<span class="dt">preProc=</span><span class="ot">NULL</span>, <span class="dt">submodels=</span><span class="ot">NULL</span>){
  <span class="kw">predict</span>(modelFit, newdata)
}

ridge$sort&lt;-function (x) x[<span class="kw">order</span>(-x$lambda), ]

ridge$label&lt;-<span class="st">&quot;Ridge&quot;</span></code></pre>
<p>Now, we can fit a ridge regression for this training dataset.</p>
<pre class="sourceCode r"><code class="sourceCode r">fitControl &lt;-<span class="st"> </span><span class="kw">trainControl</span>(
  <span class="dt">method =</span> <span class="st">&quot;repeatedcv&quot;</span>,
  <span class="dt">number =</span> <span class="dv">10</span>,
  <span class="dt">repeats =</span> <span class="dv">10</span>)

<span class="kw">set.seed</span>(<span class="dv">3245</span>)
lm.ridge &lt;-<span class="st"> </span><span class="kw">train</span>(<span class="dt">y=</span>training$crim, <span class="dt">x=</span>training[,-<span class="dv">1</span>], <span class="dt">method=</span>ridge, <span class="dt">trControl =</span> fitControl)
lm.ridge            </code></pre>
<pre><code>## Ridge 
## 
## 382 samples
##  13 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 10 times) 
## Summary of sample sizes: 343, 345, 344, 345, 343, 346, ... 
## Resampling results across tuning parameters:
## 
##   lambda  RMSE      Rsquared   RMSE SD   Rsquared SD
##   1e-03   6.344069  0.5758223  3.602768  0.1721887  
##   5e-01   6.337899  0.5766161  3.608802  0.1728289  
##   5e+00   6.325569  0.5781598  3.623083  0.1743207  
##   3e+01   6.300387  0.5815323  3.648259  0.1769855  
##   7e+01   6.281639  0.5840679  3.668541  0.1791975  
##   4e+02   6.256949  0.5862690  3.711660  0.1827676  
## 
## RMSE was used to select the optimal model using  the smallest value.
## The final value used for the model was lambda = 400.</code></pre>
<p>We can tell from looking at this result that the lambda which gives the smallest RMSE is 30.</p>
<p>Finally, evaluate the models on the test dataset. Following are the results:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">3245</span>)
lm.test &lt;-<span class="st"> </span><span class="kw">train</span>(crim ~<span class="st"> </span>.,<span class="dt">data =</span> testing, <span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>)
lm.test</code></pre>
<pre><code>## Linear Regression 
## 
## 124 samples
##  13 predictor
## 
## No pre-processing
## Resampling: Bootstrapped (25 reps) 
## Summary of sample sizes: 124, 124, 124, 124, 124, 124, ... 
## Resampling results
## 
##   RMSE      Rsquared   RMSE SD    Rsquared SD
##   4.038202  0.5724172  0.9355334  0.1103047  
## 
## </code></pre>
<pre class="sourceCode r"><code class="sourceCode r">lm.forward.test &lt;-<span class="st"> </span><span class="kw">train</span>(crim ~<span class="st"> </span>.,<span class="dt">data =</span> testing, <span class="dt">method =</span> <span class="st">&quot;leapForward&quot;</span>)
lm.forward.test</code></pre>
<pre><code>## Linear Regression with Forward Selection 
## 
## 124 samples
##  13 predictor
## 
## No pre-processing
## Resampling: Bootstrapped (25 reps) 
## Summary of sample sizes: 124, 124, 124, 124, 124, 124, ... 
## Resampling results across tuning parameters:
## 
##   nvmax  RMSE      Rsquared   RMSE SD   Rsquared SD
##   2      4.379888  0.5338787  1.138573  0.1277908  
##   3      4.369608  0.5364374  1.134358  0.1224062  
##   4      4.434249  0.5248151  1.124006  0.1247492  
## 
## RMSE was used to select the optimal model using  the smallest value.
## The final value used for the model was nvmax = 3.</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">lm.ridge.test &lt;-<span class="st"> </span><span class="kw">train</span>(<span class="dt">y=</span>testing$crim, <span class="dt">x=</span>testing[,-<span class="dv">1</span>], <span class="dt">method=</span>ridge, <span class="dt">trControl =</span> fitControl)
lm.ridge.test</code></pre>
<pre><code>## Ridge 
## 
## 124 samples
##  13 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 10 times) 
## Summary of sample sizes: 111, 112, 112, 112, 112, 112, ... 
## Resampling results across tuning parameters:
## 
##   lambda  RMSE      Rsquared   RMSE SD   Rsquared SD
##   1e-03   3.635149  0.7364990  1.981119  0.1492026  
##   5e-01   3.606367  0.7404771  1.985204  0.1465165  
##   5e+00   3.583996  0.7431323  1.983908  0.1443045  
##   3e+01   3.548088  0.7472046  1.977176  0.1418460  
##   7e+01   3.524726  0.7496966  1.973079  0.1404285  
##   4e+02   3.486704  0.7521016  1.961957  0.1371996  
## 
## RMSE was used to select the optimal model using  the smallest value.
## The final value used for the model was lambda = 400.</code></pre>
<p>This result shows that when fitting the three models, <code>lm.test</code>, <code>lm.forward.test</code>, and <code>lm.ridge.test</code>, with the test dataset, the ridge regression model gives us the best result. The ridge regression with the best hyperparameter value, lambda is 30, has the smallest RMSE.</p>
</div>



<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
